{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.1 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rshwndsz/templates/blob/master/pl-colab/template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3ZAarhZX6bg"
      },
      "source": [
        "# PL_TEMPLATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNJxTTxoX7oQ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "All that begins well, ends well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSljkR8AYIit"
      },
      "source": [
        "### Packages\n",
        "\n",
        "Install and import packages here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4FYNFxPXF2v"
      },
      "source": [
        "%%shell\n",
        "pip install -q git+https://github.com/PytorchLightning/pytorch-lightning.git@master\n",
        "pip install -q git+https://github.com/albumentations-team/albumentations\n",
        "pip install -q neptune-client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5eE9JbXNP6"
      },
      "source": [
        "# STL\n",
        "import math\n",
        "import os\n",
        "import glob\n",
        "import logging\n",
        "import getpass\n",
        "from pathlib import Path\n",
        "\n",
        "# Numerical Python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Scientific Python\n",
        "import scipy\n",
        "\n",
        "# Image processing\n",
        "from PIL import Image\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data as D\n",
        "import torchvision as tv\n",
        "import pytorch_lightning as pl\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
        "from pytorch_lightning.callbacks import (ModelCheckpoint,\n",
        "                                         EarlyStopping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HnBYDXyxPjd"
      },
      "source": [
        "### Config\n",
        "\n",
        "Important constants & magic number used all over can go here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xZIhKRLYD9-"
      },
      "source": [
        "CONSTANTS = {\n",
        "    # Random number to seed everything using pytorch lightning magic\n",
        "    'SEED': 81, \n",
        "    # Neptune API Token\n",
        "    'API_TOKEN': getpass.getpass(\"Enter Neptune.ai API Token: \"),\n",
        "}\n",
        "\n",
        "pl.seed_everything(CONSTANTS['SEED'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPnHeCE8X-j4"
      },
      "source": [
        "### Logging\n",
        "\n",
        "Logging to file and console. cuz why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gi5nbpGXkZI"
      },
      "source": [
        "# Set up logging to file\n",
        "# https://stackoverflow.com/a/23681578\n",
        "\n",
        "logging.basicConfig(\n",
        "     filename='LOG.log',\n",
        "     level=logging.INFO, \n",
        "     format= '[%(asctime)s] %(levelname)8s - %(funcName)8s() - %(message)s',\n",
        "     datefmt='%H:%M:%S'\n",
        " )\n",
        "\n",
        "# Set up logging to console\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.DEBUG)\n",
        "\n",
        "# Set a format which is simpler for console use\n",
        "formatter = logging.Formatter('%(levelname)8s - %(funcName)14s() - %(message)s')\n",
        "console.setFormatter(formatter)\n",
        "\n",
        "# Add the handler to the root logger\n",
        "logging.getLogger('').addHandler(console)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Test drive the logger\n",
        "logger.info(f\"\"\"\n",
        "            Torch: {torch.__version__}\n",
        "            Torchvision: {tv.__version__}\n",
        "            Pytorch Lightning: {pl.__version__} \n",
        "            Albumentations: {A.__version__}\n",
        "            \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9pIQ3CNX9lX"
      },
      "source": [
        "### Utils\n",
        "\n",
        "Some helper functions to make life easier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FC04tuiXybH"
      },
      "source": [
        "def download_file(url, \n",
        "                  destination_dir='./', \n",
        "                  desc=None, \n",
        "                  force=False):\n",
        "    \"\"\"Download a file from any url using requests\n",
        "    \"\"\"\n",
        "    # Convert path to pathlib object if not already\n",
        "    destination_dir = Path(destination_dir)\n",
        "    # Get filename from url\n",
        "    fname = url.split('/')[-1]\n",
        "    # Construct path to file in local machine\n",
        "    local_filepath = Path(destination_dir) / fname\n",
        "\n",
        "    if local_filepath.is_file() and not force:\n",
        "        logger.info(\"File(s) already downloaded. Use force=True to download again.\")\n",
        "        return local_filepath\n",
        "    else:\n",
        "        # Safely create nested directory - https://stackoverflow.com/a/273227\n",
        "        destination_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if desc is None:\n",
        "        desc = f\"Downloading {fname}\"\n",
        "\n",
        "    # Download large file with requests - https://stackoverflow.com/a/16696317\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        total_size_in_bytes = int(r.headers.get('content-length', 0))\n",
        "        block_size          = 1024\n",
        "        # Progress bar for downloading file - https://stackoverflow.com/a/37573701\n",
        "        pbar = tqdm(total=total_size_in_bytes, \n",
        "                    unit='iB', \n",
        "                    unit_scale=True,\n",
        "                    desc=desc)\n",
        "        with open(local_filepath, 'wb') as f:\n",
        "            for data in r.iter_content(block_size):\n",
        "                pbar.update(len(data))\n",
        "                f.write(data)\n",
        "        pbar.close()\n",
        "    return local_filepath\n",
        "\n",
        "\n",
        "def extract_file(fname, \n",
        "                 ftype=None, \n",
        "                 destination_dir=\"./\", \n",
        "                 desc=None, \n",
        "                 remove_extract=False):\n",
        "    # Convert to pathlib objects\n",
        "    fname = Path(fname)\n",
        "    destination_dir = Path(destination_dir)\n",
        "\n",
        "    # Check arguments\n",
        "    if not fname.is_file():\n",
        "        raise IOError(f\"The file {str(fname)} does not exist.\")\n",
        "    \n",
        "    # Safely create nested directory - https://stackoverflow.com/a/273227\n",
        "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if desc is None:\n",
        "        desc = f\"Extracting {str(fname.name)}\"\n",
        "\n",
        "    # Get type of extract\n",
        "    if ftype is None:\n",
        "        ftype = fname.suffix\n",
        "\n",
        "    # Extract the dataset into `destination_dir`\n",
        "    if ftype == '.tar':\n",
        "        with tarfile.open(fname) as tar:\n",
        "            pbar = tqdm(\n",
        "                iterable=tar.getmembers(), \n",
        "                total=len(tar.getmembers()), \n",
        "                desc=desc\n",
        "            )\n",
        "            # Extract files with progress bar - https://stackoverflow.com/a/53405055\n",
        "            for member in pbar:\n",
        "                tar.extract(member=member, path=destination_dir)\n",
        "\n",
        "    elif ftype == '.zip':\n",
        "        # https://stackoverflow.com/a/56970565\n",
        "        with ZipFile(fname, 'r') as zip:\n",
        "            pbar = tqdm(zip.infolist(), desc=desc)\n",
        "            for member in pbar:\n",
        "                zip.extract(member, destination_dir)\n",
        "\n",
        "    else:\n",
        "        raise IOError(f\"The suffix: {ftype} is not supported.\")\n",
        "            \n",
        "    if remove_extract:\n",
        "        # Delete the compressed dataset\n",
        "        os.remove(fname)   \n",
        "\n",
        "\n",
        "def make_grid(tensors, \n",
        "              nrow=2, \n",
        "              padding=2, \n",
        "              isNormalized=True):\n",
        "    \"\"\"Convert a list of tensors into a numpy image grid\n",
        "    \"\"\"\n",
        "    grid = tv.utils.make_grid(tensor=tensors.detach().cpu(), \n",
        "                              nrow=nrow, \n",
        "                              padding=padding, \n",
        "                              normalize= (not isNormalized))\n",
        "    if isNormalized:\n",
        "        ndgrid = grid.mul(255)\n",
        "                     .add_(0.5)\n",
        "                     .clamp_(0, 255)\n",
        "                     .permute(1, 2, 0)\n",
        "                     .numpy()\n",
        "                     .astype(np.uint16)\n",
        "    else:\n",
        "        ndgrid = grid.clamp_(0, 255)\n",
        "                     .permute(1, 2, 0)\n",
        "                     .numpy()\n",
        "                     .astype(np.uint16)\n",
        "\n",
        "    return ndgrid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhUzTCoJarur"
      },
      "source": [
        "### Downloads\n",
        "\n",
        "Download datasets, models, config files and more with curl, wget, gdown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-xReLh2arur"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-xIcmFTYSmn"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5kc0Yguarus"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua3ADa1narus"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBtEC8hGarus"
      },
      "source": [
        "### APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuxgRWpWYSGs"
      },
      "source": [
        "class GenericImageDS(D.Dataset):\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 image_glob=\"*.jpg\",\n",
        "                 train=True,\n",
        "                 transform=None,\n",
        "                 min_image_dim=256):\n",
        "        self.root = root\n",
        "        self.image_glob = image_glob\n",
        "        self.train = train\n",
        "        self.min_image_dim = min_image_dim\n",
        "\n",
        "        image_regex = os.path.join(self.root, self.image_glob)\n",
        "        self.image_paths = glob.glob(image_regex)\n",
        "        if not len(self.image_paths):\n",
        "            raise ValueError(f\"No image found using {image_regex}\")\n",
        "\n",
        "        self.transform = transform\n",
        "        # Default set of transforms if none are provided\n",
        "        if self.transform is None:\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(self.min_image_dim, self.min_image_dim, 4, True, 1),\n",
        "                A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), p=1),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "        logger.info(f\"Total samples: {len(self.image_paths)}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def download(urls, destination_dir, force=False):\n",
        "        destination_dir = Path(destination_dir)\n",
        "\n",
        "        # Check validity of arguments\n",
        "        if not destination_dir.is_dir():\n",
        "            raise ValueError(\"Provide destination_dir\")\n",
        "        if urls is None:\n",
        "            raise ValueError(\"Provide URL(s)\")\n",
        "\n",
        "        # Download & Extract\n",
        "        for url in urls:\n",
        "            fname = download_file(url, destination_dir)\n",
        "            extract_file(fname, destination_dir)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        image = np.asarray(Image.open(image_path))\n",
        "        image = self.transform(image=image)[\"image\"]\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-nvg48JYlhF"
      },
      "source": [
        "# Test drive the dataset API"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z0Lw2Zzarut"
      },
      "source": [
        "## Model Blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbTmR55Garuu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmGy6RSaYzx"
      },
      "source": [
        "## Lightning Module ü™Ñ\n",
        "\n",
        "Where it all comes together.\n",
        "Use PyTorch-lightning to organise research code in a single cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57jkQ5F5aVs_"
      },
      "source": [
        "class FinalNet(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(FinalNet, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def loss_function(self, preds, targets):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "                  \n",
        "    def train_dataloader(self):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def val_dataloader(self):\n",
        "        # TODO\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch\n",
        "        preds = self(inputs)\n",
        "        loss  = self.loss_function(preds, targets)\n",
        "\n",
        "        self.logger.experiment.log_metric('step_train_loss', loss)\n",
        "        return { 'loss': loss }\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "\n",
        "        self.logger.experiment.log_metric('epoch_train_loss', avg_loss)\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch\n",
        "        preds = self(inputs)\n",
        "        loss = self.loss_function(preds, targets)\n",
        "\n",
        "        self.logger.experiment.log_metric('step_val_loss', loss)\n",
        "        return { 'val_loss': loss }\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_val_loss = torch.stack([output['val_loss'] for output in outputs]).mean()\n",
        "\n",
        "        self.log('avg_val_loss', avg_val_loss)\n",
        "        self.logger.experiment.log_metric('epoch_val_loss', avg_val_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdb6OMKha1N3"
      },
      "source": [
        "## Training\n",
        "\n",
        "Where the fun's at."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-h2wDvKa6jQ"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvUXgrOha0op"
      },
      "source": [
        "hparams = {\n",
        "    'lr': 0.0001, \n",
        "    'batch_size': 4,\n",
        "    'max_epochs': 200,\n",
        "    'min_epochs': 10,\n",
        "    'check_val_every_n_epoch': 4,\n",
        "    'precision': 32,     # https://pytorch-lightning.readthedocs.io/en/latest/amp.html\n",
        "    'benchmark': True,\n",
        "    'deterministic': False,\n",
        "    'use_gpu': torch.cuda.is_available(),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR-mWHfha4Kg"
      },
      "source": [
        "### Bells & whistles\n",
        "\n",
        "Callbacks and other fancy stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-53FRnJxa3b8"
      },
      "source": [
        "# https://pytorch-lightning.readthedocs.io/en/latest/weights_loading.html?highlight=ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    dirpath    = \"./checkpoints/\",\n",
        "    filename   = '{epoch:03d}__{avg_val_loss:.5f}',\n",
        "    save_top_k = 5,\n",
        "    monitor    = 'avg_val_loss',\n",
        "    mode       = 'min',\n",
        "    period     = 5\n",
        ")\n",
        "\n",
        "# https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html\n",
        "early_stop_callback = EarlyStopping(\n",
        "   monitor   = 'avg_val_loss',\n",
        "   min_delta = 1e-7,\n",
        "   patience  = 3,\n",
        "   verbose   = True,\n",
        "   mode      = 'min'\n",
        ")\n",
        "\n",
        "# https://docs.neptune.ai/api-reference/neptune/experiments/index.html#neptune.experiments.Experiment\n",
        "pl_logger = NeptuneLogger(\n",
        "    api_key         = CONSTANTS['API_TOKEN'],\n",
        "    project_name    = f\"\", # TODO\n",
        "    close_after_fit = True,\n",
        "    experiment_name = '',  # TODO\n",
        "    params          = hparams,\n",
        "    # offline_model   = True,  # Uncomment for offline training\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oi_vqv6x7BD"
      },
      "source": [
        "### Instantiations\n",
        "\n",
        "Call everybody here cuz we'll be training in the next cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9yuf8HGa8E8"
      },
      "source": [
        "logger.setLevel(logging.INFO)\n",
        "pl.seed_everything(CONSTANTS['SEED'])\n",
        "\n",
        "model   = FinalNet(hparams=hparams)\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus                    = -1 if hparams['use_gpu'] else 0,\n",
        "    precision               = hparams['precision'],\n",
        "    gradient_clip_val       = hparams['gradient_clip_val'],\n",
        "    benchmark               = hparams['benchmark'],\n",
        "    deterministic           = hparams['deterministic'],\n",
        "    max_epochs              = hparams['max_epochs'],\n",
        "    min_epochs              = hparams['min_epochs'],\n",
        "    check_val_every_n_epoch = hparams['check_val_every_n_epoch'],\n",
        "    logger                  = pl_logger,\n",
        "    checkpoint_callback     = model_checkpoint,\n",
        "    callbacks               = [early_stop_callback],\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Dwn5rGarux"
      },
      "source": [
        "Some before training logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utXrRdigarux"
      },
      "source": [
        "# Log model summary\n",
        "for chunk in [x for x in str(model).split('\\n')]:\n",
        "    neptune_logger.experiment.log_text('model_summary', str(chunk))\n",
        "\n",
        "# Which GPUs where used?\n",
        "gpu_list = [f'{i}:{torch.cuda.get_device_name(i)}' \n",
        "            for i in range(torch.cuda.device_count())] \n",
        "neptune_logger.experiment.log_text('GPUs used', ', '.join(gpu_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8968ANG9bR1G"
      },
      "source": [
        "### Train \n",
        "\n",
        "Let it rip!üêâüêâ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qnj1zATbRcz"
      },
      "source": [
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOE820zzaruy"
      },
      "source": [
        "Some after training logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKV4JG7bVKc"
      },
      "source": [
        "# Log best 3 model checkpoints to Neptune\n",
        "for k in model_checkpoint.best_k_models.keys():\n",
        "    model_name = 'checkpoints/' + k.split('/')[-1]\n",
        "    neptune_logger.experiment.log_artifact(k, model_name)\n",
        "\n",
        "# Save last path\n",
        "last_model_path = f\"checkpoints/last_model--epoch={trainer.current_epoch}.ckpt\"\n",
        "trainer.save_checkpoint(last_model_path)\n",
        "neptune_logger.experiment.log_artifact(\n",
        "    last_model_path, \n",
        "    'checkpoints/' + last_model_path.split('/')[-1]\n",
        ")\n",
        "\n",
        "# Log score of the best model checkpoint\n",
        "neptune_logger.experiment.set_property(\n",
        "    'best_model_score', \n",
        "    model_checkpoint.best_model_score.tolist()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0hfgUpGc8KY"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loFyeV9QdAMG"
      },
      "source": [
        "### Get weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPgTX3xoc9mH"
      },
      "source": [
        "# Get Neptune API token\n",
        "from getpass import getpass\n",
        "api_token = getpass(\"Enter Neptune.ai API token: \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBsssHUVc-_H"
      },
      "source": [
        "# Initialize Neptune project\n",
        "import neptune\n",
        "from neptune import Session\n",
        "\n",
        "session = Session.with_default_backend(api_token=api_token)\n",
        "project = session.get_project(f\"\") # TODO\n",
        "experiment = project.get_experiments(id='')[0] # TODO\n",
        "experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Tl1389dDnu"
      },
      "source": [
        "# Download checkpoint from Neptune\n",
        "artifact_path   = 'epoch=133-avg_val_loss=1.06.ckpt'\n",
        "artifact_name   = artifact_path.split('/')[-1]\n",
        "checkpoint_dir  = os.path.join('checkpoints', 'downloads')\n",
        "checkpoint_path = os.path.join(checkpoint_dir, artifact_name)\n",
        "\n",
        "experiment.download_artifact(path=artifact_path, destination_dir=checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep_t3K3QdOf8"
      },
      "source": [
        "### Load weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK0SBDmudNma"
      },
      "source": [
        "model = FinalNet.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbQiVlludLp7"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r8XHFogdMKq"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}